{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIZrAUx57vsM"
      },
      "source": [
        "Practical 1: Sentiment Detection in Movie Reviews\n",
        "========================================\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4kXPMhyngZW"
      },
      "source": [
        "This practical concerns detecting sentiment in movie reviews. This is a typical NLP classification task.\n",
        "In [this file](https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json) (80MB) you will find 1000 positive and 1000 negative **movie reviews**.\n",
        "Each review is a **document** and consists of one or more sentences.\n",
        "\n",
        "To prepare yourself for this practical, you should\n",
        "have a look at a few of these texts to understand the difficulties of\n",
        "the task: how might one go about classifying the texts? You will write\n",
        "code that decides whether a movie review conveys positive or\n",
        "negative sentiment.\n",
        "\n",
        "Please make sure you have read the following paper:\n",
        "\n",
        ">   Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan\n",
        "(2002).\n",
        "[Thumbs up? Sentiment Classification using Machine Learning\n",
        "Techniques](https://dl.acm.org/citation.cfm?id=1118704). EMNLP.\n",
        "\n",
        "Bo Pang et al. introduced the movie review sentiment\n",
        "classification task, and the above paper was one of the first papers on\n",
        "the topic. The first version of your sentiment classifier will do\n",
        "something similar to Pang et al.'s system. If you have questions about it,\n",
        "you should resolve you doubts as soon as possible with your TA.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb7errgRASzZ"
      },
      "source": [
        "**Advice**\n",
        "\n",
        "Please read through the entire practical and familiarise\n",
        "yourself with all requirements before you start coding or otherwise\n",
        "solving the tasks. Writing clean and concise code can make the difference\n",
        "between solving the assignment in a matter of hours, and taking days to\n",
        "run all experiments.\n",
        "\n",
        "\n",
        "**Implementation**\n",
        "\n",
        "While we inserted code cells to indicate where you should implement your own code, please feel free to add/remove code blocks where you see fit (but make sure that the general structure of the assignment is preserved). Also, please keep in mind that it is always good practice to structure your code properly, e.g., by implementing separate classes and functions that can be reused.\n",
        "\n",
        "## Environment\n",
        "\n",
        "All code should be written in **Python 3**.\n",
        "This is the default in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaZnxptMJiD7",
        "outputId": "4fb682f0-e009-4983-979d-0461b2b6d723"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.5\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYZyIF7lJnGn"
      },
      "source": [
        "If you want to run code on your own computer, then download this notebook through `File -> Download .ipynb`.\n",
        "The easiest way to\n",
        "install Python is through downloading\n",
        "[Anaconda](https://www.anaconda.com/download).\n",
        "After installation, you can start the notebook by typing `jupyter notebook filename.ipynb`.\n",
        "You can also use an IDE\n",
        "such as [PyCharm](https://www.jetbrains.com/pycharm/download/) to make\n",
        "coding and debugging easier. It is good practice to create a [virtual\n",
        "environment](https://docs.python.org/3/tutorial/venv.html) for this\n",
        "project, so that any Python packages don’t interfere with other\n",
        "projects.\n",
        "\n",
        "\n",
        "**Learning Python 3**\n",
        "\n",
        "If you are new to Python 3, you may want to check out a few of these resources:\n",
        "- https://learnxinyminutes.com/docs/python3/\n",
        "- https://www.learnpython.org/\n",
        "- https://docs.python.org/3/tutorial/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hok-BFu9lGoK"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import sys\n",
        "from subprocess import call\n",
        "from nltk import FreqDist\n",
        "from nltk.util import ngrams\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import sklearn as sk\n",
        "#from google.colab import drive\n",
        "import pickle\n",
        "import json\n",
        "from collections import Counter\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "import scipy.sparse as sp\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXWyGHwE-ieQ"
      },
      "source": [
        "## Loading the data\n",
        "\n",
        "**Download the sentiment lexicon and the movie reviews dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lm-rakqtlMOT",
        "outputId": "02f9dfa6-04af-4ce4-f6b0-1d4d83314b79"
      },
      "outputs": [],
      "source": [
        "# download sentiment lexicon\n",
        "#!wget https://gist.githubusercontent.com/bastings/d6f99dcb6c82231b94b013031356ba05/raw/f80a0281eba8621b122012c89c8b5e2200b39fd6/sent_lexicon\n",
        "# download review data\n",
        "#!wget https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkPwuHp5LSuQ"
      },
      "source": [
        "**Load the movie reviews.**\n",
        "\n",
        "Each word in a review comes with its part-of-speech tag. For documentation on POS-tags, see https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "careEKj-mRpl",
        "outputId": "3f3c75a8-deea-4bcc-ff66-664461ca73b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of reviews: 2000 \n",
            "\n",
            "0 NEG 29\n",
            "Two/CD teen/JJ couples/NNS go/VBP to/TO a/DT church/NN party/NN ,/, drink/NN and/CC then/RB drive/NN ./.\n",
            "1 NEG 11\n",
            "Damn/JJ that/IN Y2K/CD bug/NN ./.\n",
            "2 NEG 24\n",
            "It/PRP is/VBZ movies/NNS like/IN these/DT that/WDT make/VBP a/DT jaded/JJ movie/NN viewer/NN thankful/JJ for/IN the/DT invention/NN of/IN the/DT Timex/NNP IndiGlo/NNP watch/NN ./.\n",
            "3 NEG 19\n",
            "QUEST/NN FOR/IN CAMELOT/NNP ``/`` Quest/NNP for/IN Camelot/NNP ''/'' is/VBZ Warner/NNP Bros./NNP '/POS first/JJ feature-length/JJ ,/, fully-animated/JJ attempt/NN to/TO steal/VB clout/NN from/IN Disney/NNP 's/POS cartoon/NN empire/NN ,/, but/CC the/DT mouse/NN has/VBZ no/DT reason/NN to/TO be/VB worried/VBN ./.\n",
            "4 NEG 38\n",
            "Synopsis/NNPS :/: A/DT mentally/RB unstable/JJ man/NN undergoing/VBG psychotherapy/NN saves/VBZ a/DT boy/NN from/IN a/DT potentially/RB fatal/JJ accident/NN and/CC then/RB falls/VBZ in/IN love/NN with/IN the/DT boy/NN 's/POS mother/NN ,/, a/DT fledgling/NN restauranteur/NN ./.\n",
            "\n",
            "Number of word types: 47743\n",
            "Number of word tokens: 1512359\n",
            "\n",
            "Most common tokens:\n",
            "         , :    77842\n",
            "       the :    75948\n",
            "         . :    59027\n",
            "         a :    37583\n",
            "       and :    35235\n",
            "        of :    33864\n",
            "        to :    31601\n",
            "        is :    25972\n",
            "        in :    21563\n",
            "        's :    18043\n",
            "        it :    15904\n",
            "      that :    15820\n",
            "     -rrb- :    11768\n",
            "     -lrb- :    11670\n",
            "        as :    11312\n",
            "      with :    10739\n",
            "       for :     9816\n",
            "       his :     9542\n",
            "      this :     9497\n",
            "      film :     9404\n"
          ]
        }
      ],
      "source": [
        "# file structure:\n",
        "# [\n",
        "#  {\"cv\": integer, \"sentiment\": str, \"content\": list}\n",
        "#  {\"cv\": integer, \"sentiment\": str, \"content\": list}\n",
        "#   ..\n",
        "# ]\n",
        "# where `content` is a list of sentences,\n",
        "# with a sentence being a list of (token, pos_tag) pairs.\n",
        "\n",
        "\n",
        "with open(\"reviews.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  reviews = json.load(f)\n",
        "\n",
        "print(\"Total number of reviews:\", len(reviews), '\\n')\n",
        "\n",
        "def print_sentence_with_pos(s):\n",
        "  print(\" \".join(\"%s/%s\" % (token, pos_tag) for token, pos_tag in s))\n",
        "\n",
        "for i, r in enumerate(reviews):\n",
        "  print(r[\"cv\"], r[\"sentiment\"], len(r[\"content\"]))  # cv, sentiment, num sents\n",
        "  print_sentence_with_pos(r[\"content\"][0])\n",
        "  if i == 4:\n",
        "    break\n",
        "\n",
        "c = Counter()\n",
        "for review in reviews:\n",
        "  for sentence in review[\"content\"]:\n",
        "    for token, pos_tag in sentence:\n",
        "      c[token.lower()] += 1\n",
        "\n",
        "print(\"\\nNumber of word types:\", len(c))\n",
        "print(\"Number of word tokens:\", sum(c.values()))\n",
        "\n",
        "print(\"\\nMost common tokens:\")\n",
        "for token, count in c.most_common(20):\n",
        "  print(\"%10s : %8d\" % (token, count))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6PWaEoh8B34"
      },
      "source": [
        "# (1) Lexicon-based approach (3.5pts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsTSMb6ma4E8"
      },
      "source": [
        "A traditional approach to classify documents according to their sentiment is the lexicon-based approach. To implement this approach, you need a **sentiment lexicon**, i.e., a list of words annotated with a sentiment label (e.g., positive and negative, or a score from 0 to 5).\n",
        "\n",
        "In this practical, you will use the sentiment\n",
        "lexicon released by Wilson et al. (2005).\n",
        "\n",
        "> Theresa Wilson, Janyce Wiebe, and Paul Hoffmann\n",
        "(2005). [Recognizing Contextual Polarity in Phrase-Level Sentiment\n",
        "Analysis](http://www.aclweb.org/anthology/H/H05/H05-1044.pdf). HLT-EMNLP.\n",
        "\n",
        "Pay attention to all the information available in the sentiment lexicon. The field *word1* contains the lemma, *priorpolarity* contains the sentiment label (positive, negative, both, or neutral), *type* gives you the magnitude of the word's sentiment (strong or weak), and *pos1* gives you the part-of-speech tag of the lemma. Some lemmas can have multiple part-of-speech tags and thus multiple entries in the lexicon. The path of the lexicon file is `\"sent_lexicon\"`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ogq0Eq2hQglh",
        "outputId": "aa3ab42e-18ca-4874-ca07-fc9a30099a8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type=weaksubj len=1 word1=abandoned pos1=adj stemmed1=n priorpolarity=negative\n",
            "type=weaksubj len=1 word1=abandonment pos1=noun stemmed1=n priorpolarity=negative\n",
            "type=weaksubj len=1 word1=abandon pos1=verb stemmed1=y priorpolarity=negative\n",
            "type=strongsubj len=1 word1=abase pos1=verb stemmed1=y priorpolarity=negative\n",
            "type=strongsubj len=1 word1=abasement pos1=anypos stemmed1=y priorpolarity=negative\n"
          ]
        }
      ],
      "source": [
        "with open(\"sent_lexicon\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  line_cnt = 0\n",
        "  for line in f:\n",
        "    print(line.strip())\n",
        "    line_cnt += 1\n",
        "    if line_cnt > 4:\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mml4nOtIUBhn"
      },
      "source": [
        "Lexica such as this can be used to solve\n",
        "the classification task without using Machine Learning. For example, one might look up every word $w_1 ... w_n$ in a document, and compute a **binary score**\n",
        "$S_{binary}$ by counting how many words have a positive or a\n",
        "negative label in the sentiment lexicon $SLex$.\n",
        "\n",
        "$$S_{binary}(w_1 w_2 ... w_n) = \\sum_{i = 1}^{n}\\text{sign}(SLex\\big[w_i\\big])$$\n",
        "\n",
        "where $\\text{sign}(SLex\\big[w_i\\big])$ refers to the polarity of $w_i$.\n",
        "\n",
        "**Threshold.** On average, there are more positive than negative words per review (~7.13 more positive than negative per review) to take this bias into account you should use a threshold of **8** (roughly the bias itself) to make it harder to classify as positive.\n",
        "\n",
        "$$\n",
        "\\text{classify}(S_{binary}(w_1 w_2 ... w_n)) = \\bigg\\{\\begin{array}{ll}\n",
        "        \\text{positive} & \\text{if } S_{binary}(w_1w_2...w_n) > threshold\\\\\n",
        "        \\text{negative} & \\text{otherwise}\n",
        "        \\end{array}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOFnMvbeeZrc"
      },
      "source": [
        "#### (Q1.1) Implement this approach and report its classification accuracy. (1 pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ED2aTEYutW1-"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "def count_pos_vs_neg(content, lexicon, strong_weight=1):\n",
        "  \"\"\"\n",
        "  Counts the number of positive and negative words in content and returns the\n",
        "  weighted difference between them (negative value means more negative words).\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  content: list\n",
        "    A list of sentences forming the content of a review. Each sentence is a\n",
        "    list whose elements are (token, PoS tag) pairs.\n",
        "\n",
        "  lexicon: dict\n",
        "    A dictionary mapping tokens to (magnitude, sentiment) pairs.\n",
        "\n",
        "  strong_weight: int, optional\n",
        "    How strongly tokens with strong magnitude are weighted relative to tokens\n",
        "    with weak magnitude. A value of 1 discards magnitude information.\n",
        "    default 1.\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  score: int\n",
        "    The difference between the number of positive and negative words, with\n",
        "    strong words weighted according to strong_weight.\n",
        "  \"\"\"\n",
        "  score = 0\n",
        "  for sentence in content:\n",
        "    for token, _ in sentence:\n",
        "      type_, polarity = lexicon.get(token.lower(), (None, None))\n",
        "      sign = 0\n",
        "      weight = strong_weight if type_ == 'strongsubj' else 1\n",
        "      if polarity == 'positive':\n",
        "        sign = 1\n",
        "      elif polarity == 'negative':\n",
        "        sign = -1\n",
        "      score += sign * weight\n",
        "  return score\n",
        "\n",
        "def lexicon_classify(reviews, lexicon, threshold=8, strong_weight=1):\n",
        "  \"\"\"\n",
        "  For a given review, predict the sentiment based on the number of positive and\n",
        "  negative words in the review.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  reviews: list\n",
        "    A list of reviews, where each review is a dictionary containing at least the\n",
        "    key \"content\". The value of \"content\" is a list of sentences forming the\n",
        "    content of a review. Each sentence is a list whose elements are\n",
        "    (token, PoS tag) pairs.\n",
        "\n",
        "  lexicon: dict\n",
        "    A dictionary mapping tokens to (magnitude, sentiment) pairs.\n",
        "\n",
        "  threshold: int, optional\n",
        "    If the number of positive words minus the number of negative words is above\n",
        "    the threshold, the review is predicted to be positive. Otherwise, the review\n",
        "    is predicted to be negative.\n",
        "    default: 8.\n",
        "\n",
        "  strong_weight: int, optional\n",
        "    How strongly tokens with strong magnitude are weighted relative to tokens\n",
        "    with weak magnitude. A value of 1 discards magnitude information.\n",
        "    default: 1.\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  predictions: array\n",
        "    An array of predicted sentiment scores, where 1 is positive and 0 is negative.\n",
        "  \"\"\"\n",
        "  predictions = []\n",
        "  i=0\n",
        "  for r in reviews:\n",
        "    i += 1\n",
        "    score = count_pos_vs_neg(r['content'], lexicon, strong_weight) # Count words\n",
        "    if score > threshold: # Add prediction\n",
        "      predictions.append(1)\n",
        "    else:\n",
        "      predictions.append(0)\n",
        "  return np.array(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dKB-r2Uwecc5"
      },
      "outputs": [],
      "source": [
        "# Create dictionary of sentiment lexicon\n",
        "sentiment_lexicon = dict() # {word1: (type, priorpolarity)}\n",
        "with open(\"sent_lexicon\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  for line in f:\n",
        "    entry = [item.split('=')[1] for item in line.split()]\n",
        "    token = entry[2].lower()\n",
        "    if token in sentiment_lexicon:\n",
        "      # If token is already konwn with different sentiment, make sentiment 'both'\n",
        "      if entry[5] != sentiment_lexicon[token][1]:\n",
        "        sentiment_lexicon[token] = (entry[0], 'both')\n",
        "    else: # Add new token\n",
        "      sentiment_lexicon[token] = (entry[0], entry[5])\n",
        "\n",
        "# Get predictions and ground truths\n",
        "token_predictions = lexicon_classify(reviews, sentiment_lexicon, 8, 1)\n",
        "ground_truths = np.array([1 if r['sentiment'] == 'POS' else 0 for r in reviews])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy528EUTphz5",
        "outputId": "af80eee2-d765-4c92-b344-218584e7f20c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.68\n"
          ]
        }
      ],
      "source": [
        "# token_results should be a list of binary indicators; for example [1, 0, 1, ...]\n",
        "# where 1 indicates a correct classification and 0 an incorrect classification.\n",
        "token_results = token_predictions == ground_truths\n",
        "token_accuracy = np.mean(token_results)\n",
        "print(\"Accuracy: %0.2f\" % token_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twox0s_3eS0V"
      },
      "source": [
        "As the sentiment lexicon also has information about the **magnitude** of\n",
        "sentiment (e.g., *“excellent\"* has the same sentiment _polarity_ as *“good\"* but it has a higher magnitude), we can take a more fine-grained approach by adding up all\n",
        "sentiment scores, and deciding the polarity of the movie review using\n",
        "the sign of the weighted score $S_{weighted}$.\n",
        "\n",
        "$$S_{weighted}(w_1w_2...w_n) = \\sum_{i = 1}^{n}SLex\\big[w_i\\big]$$\n",
        "\n",
        "\n",
        "Make sure you define an appropriate threshold for this approach.\n",
        "\n",
        "#### (Q1.2) Now incorporate magnitude information and report the classification accuracy. Don't forget to use the threshold. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qG3hUDnPtkhS",
        "outputId": "918afaa0-f675-4a5e-f27b-ff4647f5c484"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bias without magnitude information: 6.82\n",
            "Bias with magnitude information: 11.20\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "# Sanity check: compute average difference between positive and negative words without\n",
        "# magnitude information.\n",
        "score = 0\n",
        "for r in reviews:\n",
        "  score += count_pos_vs_neg(r['content'], sentiment_lexicon, 1)\n",
        "print(f'Bias without magnitude information: {score / len(reviews):.2f}')\n",
        "\n",
        "# Compute average difference between positive and negative words with magnitude information.\n",
        "score = 0\n",
        "for r in reviews:\n",
        "  score += count_pos_vs_neg(r['content'], sentiment_lexicon, 2)\n",
        "print(f'Bias with magnitude information: {score / len(reviews):.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-o8Lg-ErsvSM"
      },
      "outputs": [],
      "source": [
        "# Get predictions. Threshold chosen based on results above.\n",
        "magnitude_predictions = lexicon_classify(reviews, sentiment_lexicon, 12, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vVk7CvDpyka",
        "outputId": "19564877-e528-4690-9b5b-1d1d429837bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.69\n"
          ]
        }
      ],
      "source": [
        "magnitude_results = magnitude_predictions == ground_truths\n",
        "magnitude_accuracy = np.mean(magnitude_results)\n",
        "print(\"Accuracy: %0.2f\" % magnitude_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9SHoGPfsAHV"
      },
      "source": [
        "#### (Q.1.3) Make a barplot of the two results (0.5pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "8LgBcYcXsEk3",
        "outputId": "aa30ef64-9e53-4a87-ba81-d64d6d3ee06b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "plt.bar(['token', 'magnitude'], [token_accuracy, magnitude_accuracy])\n",
        "plt.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNhS8OCVxMHd"
      },
      "source": [
        "#### (Q1.4) A better threshold (1pt)\n",
        "Above we have defined a threshold to account for an inherent bias in the dataset: there are more positive than negative words per review.\n",
        "However, that threshold does not take into account *document length*. Explain why this is a problem and implement an alternative way to compute the threshold."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo7gk1I-omLI"
      },
      "source": [
        "*Answer*\n",
        "\n",
        "Longer reviews will on average have more positive or negative words, and will therefore contribute more to the average number of positive and negative words than shorter reviews. Hence the fact that there are more positive than negative words per review does not necessarily mean that reviews with slightly more positive than negative words are mostly negative reviews. It might also be because positive reviews are longer than negative reviews on average, and thus contribute more to the difference between the number of positive and negative words. As a result, short positive reviews might not reach the threshold and be classified as negative.\n",
        "\n",
        "As an alternative, we can compute the median difference between the number of possitive and negative words. This will make sure that longer reviews don't count more than shorter ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dwt0B8h8aKjr",
        "outputId": "4c1f6504-afd0-4fa7-9e51-a84b3878c2a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Median difference without magnitude information : 6.0\n",
            "Median difference with magnitude information : 9.0\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "# We find the median difference instead of the mean difference\n",
        "\n",
        "def median(l):\n",
        "  \"\"\"Finds median value in an (unsorted) list of numbers.\"\"\"\n",
        "  l = sorted(l)\n",
        "  if len(l) % 2 == 1:\n",
        "    return l[len(l) // 2]\n",
        "  return (l[len(l) // 2 - 1] + l[len(l) // 2]) / 2\n",
        "\n",
        "# Find median difference without magnitude information\n",
        "differences = []\n",
        "for r in reviews:\n",
        "  difference = count_pos_vs_neg(r['content'], sentiment_lexicon, 1)\n",
        "  differences.append(difference)\n",
        "token_median = median(differences)\n",
        "print(\"Median difference without magnitude information :\", token_median)\n",
        "\n",
        "# Find median difference with magnitude information\n",
        "differences = []\n",
        "for r in reviews:\n",
        "  difference = count_pos_vs_neg(r['content'], sentiment_lexicon, 2)\n",
        "  differences.append(difference)\n",
        "magnitude_median = median(differences)\n",
        "print(\"Median difference with magnitude information :\", magnitude_median)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76BjBt5JwfBq",
        "outputId": "5ab920ad-5e06-4f04-fe0b-fa9c90b87fa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy without magnitude information: 0.67\n",
            "Accuracy with magnitude information: 0.68\n"
          ]
        }
      ],
      "source": [
        "# Test with new thresholds\n",
        "token_predictions = lexicon_classify(reviews, sentiment_lexicon, 6, 1)\n",
        "token_results = token_predictions == ground_truths\n",
        "token_accuracy = np.mean(token_results)\n",
        "print(\"Accuracy without magnitude information: %0.2f\" % token_accuracy)\n",
        "\n",
        "magnitude_predictions = lexicon_classify(reviews, sentiment_lexicon, 9, 2)\n",
        "magnitude_results = magnitude_predictions == ground_truths\n",
        "magnitude_accuracy = np.mean(magnitude_results)\n",
        "print(\"Accuracy with magnitude information: %0.2f\" % magnitude_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LibV4nR89BXb"
      },
      "source": [
        "# (2) Naive Bayes (9.5pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnF9adQnuwia"
      },
      "source": [
        "\n",
        "Your second task is to program a simple Machine Learning approach that operates\n",
        "on a simple Bag-of-Words (BoW) representation of the text data, as\n",
        "described by Pang et al. (2002). In this approach, the only features we\n",
        "will consider are the words in the text themselves, without bringing in\n",
        "external sources of information. The BoW model is a popular way of\n",
        "representing texts as vectors, making it\n",
        "easy to apply classical Machine Learning algorithms on NLP tasks.\n",
        "However, the BoW representation is also very crude, since it discards\n",
        "all information related to word order and grammatical structure in the\n",
        "original text—as the name suggests.\n",
        "\n",
        "## Writing your own classifier (4pts)\n",
        "\n",
        "Write your own code to implement the Naive Bayes (NB) classifier. As\n",
        "a reminder, the Naive Bayes classifier works according to the following\n",
        "equation:\n",
        "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} P(c|\\bar{f}) = \\operatorname*{arg\\,max}_{c \\in C} P(c)\\prod^n_{i=1} P(f_i|c)$$\n",
        "where $C = \\{ \\text{POS}, \\text{NEG} \\}$ is the set of possible classes,\n",
        "$\\hat{c} \\in C$ is the most probable class, and $\\bar{f}$ is the feature\n",
        "vector. Remember that we use the log of these probabilities when making\n",
        "a prediction:\n",
        "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} \\Big\\{\\log P(c) + \\sum^n_{i=1} \\log P(f_i|c)\\Big\\}$$\n",
        "\n",
        "You can find more details about Naive Bayes in [Jurafsky &\n",
        "Martin](https://web.stanford.edu/~jurafsky/slp3/). You can also look at\n",
        "this helpful\n",
        "[pseudo-code](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html).\n",
        "\n",
        "*Note: this section and the next aim to put you in a position to replicate\n",
        "    Pang et al.'s Naive Bayes results. However, your numerical results\n",
        "    will differ from theirs, as they used different data.*\n",
        "\n",
        "**You must write the Naive Bayes training and prediction code from\n",
        "scratch.** You will not be given credit for using off-the-shelf Machine\n",
        "Learning libraries.\n",
        "\n",
        "The data contains the text of the reviews, where each document consists\n",
        "of the sentences in the review, the sentiment of the review and an index\n",
        "(cv) that you will later use for cross-validation. The\n",
        "text has already been tokenised and POS-tagged for you. Your algorithm\n",
        "should read in the text, **lowercase it**, store the words and their\n",
        "frequencies in an appropriate data structure that allows for easy\n",
        "computation of the probabilities used in the Naive Bayes algorithm, and\n",
        "then make predictions for new instances.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEpyQSBSkb33"
      },
      "source": [
        "#### (Q2.1) Unseen words (1pt)\n",
        "The presence of words in the test dataset that\n",
        "have not been seen during training can cause probabilities in the Naive Bayes classifier to equal $0$.\n",
        "These can be words which are unseen in both positive and negative training reviews (case 1), but also words which are seen in reviews _of only one sentiment class_ in the training dataset (case 2). In both cases, **you should skip these words for both classes at test time**.  What would be the problem instead with skipping words only for one class in case 2?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BanFiYYnoxDW"
      },
      "source": [
        "*Answer*\n",
        "\n",
        "Say that some word appears in class 1 but does not appear in class 2 in the training data. If we were to skip it at test time for class 2, but not skip it for class 1, this would disadvantage class 1. This is because the likelihood for class 1 would be multiplied with an extra probability, making it smaller, while the likelihood for class 2 would be unchanged. This means that class 1 would become less likely than class 2, because the document has a word that only appeared in class 1. This is clearly undesirable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsZRhaI3WvzC"
      },
      "source": [
        "#### (Q2.2) Train your classifier on (positive and negative) reviews with cv-value 000-899, and test it on the remaining (positive and negative) reviews cv900–cv999.  Report results using classification accuracy as your evaluation metric. Your  features are the word vocabulary. The value of a feature is the count of that feature (word) in the document. (2pts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7zaJYGFvIJ3",
        "outputId": "cc583c9e-eaa5-48af-9c5d-8dba6d9508f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes accuracy: 0.825\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "class NaiveBayesClassifier():\n",
        "  '''\n",
        "  Creates a Naive Bayes classifier. Contains a fit and predict method.\n",
        "  When fitting, the classifier counts the number of documents and tokens in each class.\n",
        "  When predicting, the classifier predicts the class with the highest posterior probability.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    '''\n",
        "    Initialize the classifier.\n",
        "    '''\n",
        "    self.doc_counter = Counter() # Counts how often each class occurs\n",
        "    # Counts how often each token occurs per class\n",
        "    self.token_counter = {'POS': Counter(), 'NEG': Counter()}\n",
        "    self.total_tokens = dict() # The total number of tokens per class\n",
        "\n",
        "  def fit(self, reviews):\n",
        "    '''\n",
        "    Fit the classifier to the given reviews.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    reviews: list\n",
        "      A list of reviews, where each review is a dictionary with keys \"cv\",\n",
        "      \"sentiment\", and \"content\". The value of \"content\" is a list of sentences\n",
        "      forming the content of a review. Each sentence is a list of tokens.\n",
        "    '''\n",
        "    for r in reviews:\n",
        "      self.doc_counter[r['sentiment']] += 1 # Count document\n",
        "      for sentence in r['content']:\n",
        "        for token in sentence:\n",
        "          self.token_counter[r['sentiment']][token] += 1 # Count token\n",
        "    # Total number of token occurences per class\n",
        "    self.total_tokens['POS'] = self.token_counter['POS'].total()\n",
        "    self.total_tokens['NEG'] = self.token_counter['NEG'].total()\n",
        "\n",
        "  def predict(self, content):\n",
        "    '''\n",
        "    Predict the sentiment of the given content.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    content: list\n",
        "      A list of sentences, where each sentence list of tokens.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    prediction: int\n",
        "      A predicted sentiment, \"POS\" or \"NEG\".\n",
        "    '''\n",
        "    # Prior probabilities of each class\n",
        "    p_pos = self.doc_counter['POS'] / self.doc_counter.total()\n",
        "    p_neg = self.doc_counter['NEG'] / self.doc_counter.total()\n",
        "    # Log posteriors per class\n",
        "    pos_posterior = np.log(p_pos)\n",
        "    neg_posterior = np.log(p_neg)\n",
        "    for sentence in content:\n",
        "      for token in sentence:\n",
        "        # Only include tokens that were seen for both classes during training\n",
        "        if token in self.token_counter['POS'] and token in self.token_counter['NEG']:\n",
        "          # Update log posteriors\n",
        "          pos_posterior += np.log(self.token_counter['POS'][token] / self.total_tokens['POS'])\n",
        "          neg_posterior += np.log(self.token_counter['NEG'][token] / self.total_tokens['NEG'])\n",
        "    if pos_posterior > neg_posterior:\n",
        "      return 'POS'\n",
        "    return 'NEG'\n",
        "\n",
        "# Create dataset with lowered tokens without POS tags\n",
        "reviews_no_pos = deepcopy(reviews)\n",
        "for r in reviews_no_pos:\n",
        "  for i, sentence in enumerate(r['content']):\n",
        "    # Lower tokens, remove POS tags\n",
        "    sentence_no_pos = [token.lower() for token, _ in sentence]\n",
        "    r['content'][i] = sentence_no_pos\n",
        "\n",
        "# Train and test Naive Bayes classifier\n",
        "NB_classifier = NaiveBayesClassifier()\n",
        "NB_classifier.fit(reviews_no_pos[:900] + reviews_no_pos[1000:1900])\n",
        "test_reviews = reviews_no_pos[900:1000] + reviews_no_pos[1900:]\n",
        "NB_results = [NB_classifier.predict(r['content']) ==\n",
        "                                  r['sentiment'] for r in test_reviews]\n",
        "NB_accuracy = sum(NB_results) / len(NB_results)\n",
        "print(\"Naive Bayes accuracy: %0.3f\" % NB_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0INK-PBoM6CB"
      },
      "source": [
        "#### (Q2.3) Would you consider accuracy to also be a good way to evaluate your classifier in a situation where 90% of your data instances are of positive movie reviews? (1pt)\n",
        "\n",
        "Simulate this scenario by keeping the positive reviews\n",
        "data unchanged, but only using negative reviews cv000–cv089 for\n",
        "training, and cv900–cv909 for testing. Calculate the classification\n",
        "accuracy, and explain what changed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFbcsYlipBAw"
      },
      "source": [
        "*Answer*\n",
        "\n",
        "No, accuracy would not be a good evaluation measure in that scenario. A classifier that classifies all data points as positive would get 90% accuracy, giving the impression that it works well, while it is actually useless.\n",
        "\n",
        "Simulating this situation resulted in a drastically lower accuracy of 60%. The model probably had too few negative examples to learn how to properly differentiate between the two classes. Also, the vocabulary of the negative training reviews must be very small. As only tokens that occur in both classes are used for prediction, this means it skips a lot of relevant words, making predictions worse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWDkt5ZrrFGp",
        "outputId": "e5726db5-6b0a-4db3-e1b3-3fbb7028ffc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes accuracy: 0.600\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "NB_classifier = NaiveBayesClassifier()\n",
        "NB_classifier.fit(reviews_no_pos[:90] + reviews_no_pos[1000:1900])\n",
        "test_reviews = reviews_no_pos[900:910] + reviews_no_pos[1900:]\n",
        "NB_results = [NB_classifier.predict(r['content']) ==\n",
        "                                  r['sentiment'] for r in test_reviews]\n",
        "NB_accuracy = sum(NB_results) / len(NB_results)\n",
        "print(\"Naive Bayes accuracy: %0.3f\" % NB_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wJzcHX3WUDm"
      },
      "source": [
        "## Smoothing (1pt)\n",
        "\n",
        "As mentioned above, the presence of words in the test dataset that\n",
        "have not been seen during training can cause probabilities in the Naive\n",
        "Bayes classifier to be $0$, thus making that particular test instance\n",
        "undecidable. The standard way to mitigate this effect (as well as to\n",
        "give more clout to rare words) is to use smoothing, in which the\n",
        "probability fraction\n",
        "\n",
        "$$ \\frac{\\text{count}(w_i, c)}{\\sum\\limits_{w\\in V} \\text{count}(w, c)}$$ \n",
        "for a word $w_i$ becomes\n",
        "$$\\frac{\\text{count}(w_i, c) + \\text{smoothing}(w_i)}{\\sum\\limits_{w\\in V} \\text{count}(w, c) + \\sum\\limits_{w \\in V} \\text{smoothing}(w)}$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBNIcbwUWphC"
      },
      "source": [
        "#### (Q2.4) Implement Laplace feature smoothing (1pt)\n",
        "Implement Laplace smoothing, i.e., smoothing with a constant value ($smoothing(w) = \\kappa, \\forall w \\in V$), in your Naive\n",
        "Bayes classifier’s code, and report the accuracy.\n",
        "Use $\\kappa = 1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g03yflCc9kpW",
        "outputId": "3b6b92b3-539c-4a60-b84f-d8d95a09c37a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes accuracy: 0.830\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "class NaiveBayesClassifier():\n",
        "  '''\n",
        "  Creates a Naive Bayes classifier with Laplace smoothing. Contains a fit and predict method.\n",
        "  When fitting, the classifier counts the number of documents and tokens in each class.\n",
        "  When predicting, the classifier predicts the class with the highest posterior probability.\n",
        "  '''\n",
        "  def __init__(self, k=1):\n",
        "    '''\n",
        "    Initialize the classifier.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    k: int, optional\n",
        "      The smoothing parameter.\n",
        "      default: 1\n",
        "    '''\n",
        "    self.doc_counter = Counter() # Counts how often each class occurs\n",
        "    # Counts how often each token occurs per class\n",
        "    self.token_counter = {'POS': Counter(), 'NEG': Counter()}\n",
        "    self.total_tokens = dict() # The total number of tokens per class\n",
        "    self.k = k # Smoothing constant\n",
        "\n",
        "  def __class__(self):\n",
        "    return NaiveBayesClassifier(self.k)\n",
        "\n",
        "  def fit(self, reviews):\n",
        "    '''\n",
        "    Fit the classifier to the given reviews.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    reviews: list\n",
        "      A list of reviews, where each review is a dictionary with keys \"cv\",\n",
        "      \"sentiment\", and \"content\". The value of \"content\" is a list of sentences\n",
        "      forming the content of a review. Each sentence is a list of tokens.\n",
        "    '''\n",
        "    for r in reviews:\n",
        "      self.doc_counter[r['sentiment']] += 1 # Count document\n",
        "      for sentence in r['content']:\n",
        "        for token in sentence:\n",
        "          self.token_counter[r['sentiment']][token] += 1 # Count token\n",
        "    # Total number of token per class + vocabulary size per class * smoothing constant\n",
        "    self.total_tokens['POS'] = self.token_counter['POS'].total() + len(self.token_counter['POS']) * self.k\n",
        "    self.total_tokens['NEG'] = self.token_counter['NEG'].total() + len(self.token_counter['NEG']) * self.k\n",
        "\n",
        "  def predict(self, content):\n",
        "    '''\n",
        "    Predict the sentiment of the given content.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    content: list\n",
        "      A list of sentences, where each sentence list of tokens.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    prediction: int\n",
        "      A predicted sentiment, \"POS\" or \"NEG\".\n",
        "    '''\n",
        "    # Prior probabilities of each class\n",
        "    p_pos = self.doc_counter['POS'] / self.doc_counter.total()\n",
        "    p_neg = self.doc_counter['NEG'] / self.doc_counter.total()\n",
        "    # Log posteriors per class\n",
        "    pos_posterior = np.log(p_pos)\n",
        "    neg_posterior = np.log(p_neg)\n",
        "    for sentence in content:\n",
        "      for token in sentence:\n",
        "        # Only include tokens that were seen for at least one class during training\n",
        "        if token in self.token_counter['POS'] or token in self.token_counter['NEG']:\n",
        "          # Update log posteriors using smoothing constant\n",
        "          # (demoninator already includes smoothing term)\n",
        "          pos_posterior += np.log((self.token_counter['POS'][token] + self.k) / self.total_tokens['POS'])\n",
        "          neg_posterior += np.log((self.token_counter['NEG'][token] + self.k) / self.total_tokens['NEG'])\n",
        "    if pos_posterior > neg_posterior:\n",
        "      return 'POS'\n",
        "    return 'NEG'\n",
        "\n",
        "# Train and test smoothed Naive Bayes classifier\n",
        "NB_classifier = NaiveBayesClassifier()\n",
        "NB_classifier.fit(reviews_no_pos[:900] + reviews_no_pos[1000:1900])\n",
        "test_reviews = reviews_no_pos[900:1000] + reviews_no_pos[1900:]\n",
        "NB_results = [NB_classifier.predict(r['content']) ==\n",
        "                                  r['sentiment'] for r in test_reviews]\n",
        "NB_accuracy = sum(NB_results) / len(NB_results)\n",
        "print(\"Naive Bayes accuracy: %0.3f\" % NB_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiGcgwba87D5"
      },
      "source": [
        "## Cross-Validation (1.5pts)\n",
        "\n",
        "A serious danger in using Machine Learning on small datasets, with many\n",
        "iterations of slightly different versions of the algorithms, is ending up with Type III errors, also called the “testing hypotheses\n",
        "suggested by the data” errors. This type of error occurs when we make\n",
        "repeated improvements to our classifiers by playing with features and\n",
        "their processing, but we don’t get a fresh, never-before seen test\n",
        "dataset every time. Thus, we risk developing a classifier that gets better\n",
        "and better on our data, but only gets worse at generalizing to new, unseen data. In other words, we risk developping a classifier that overfits.\n",
        "\n",
        "A simple method to guard against Type III errors is to use\n",
        "Cross-Validation. In **N-fold Cross-Validation**, we divide the data into N\n",
        "distinct chunks, or folds. Then, we repeat the experiment N times: each\n",
        "time holding out one of the folds for testing, training our classifier\n",
        "on the remaining N - 1 data folds, and reporting performance on the\n",
        "held-out fold. We can use different strategies for dividing the data:\n",
        "\n",
        "-   Consecutive splitting:\n",
        "  - cv000–cv099 = Split 1\n",
        "  - cv100–cv199 = Split 2\n",
        "  - etc.\n",
        "  \n",
        "-   Round-robin splitting (mod 10):\n",
        "  - cv000, cv010, cv020, … = Split 1\n",
        "  - cv001, cv011, cv021, … = Split 2\n",
        "  - etc.\n",
        "\n",
        "-   Random sampling/splitting\n",
        "  - Not used here (but you may choose to split this way in a non-educational situation)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OeLcbSauGtR"
      },
      "source": [
        "#### (Q2.5) Write the code to implement 10-fold cross-validation using round-robin splitting for your Naive Bayes classifier from Q2.4 and compute the 10 accuracies. Report the final performance, which is the average of the performances per fold. If all splits perform equally well, this is a good sign. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3KeCGPa7Nuzx"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "def kfold_NB(classifier, reviews, k=10):\n",
        "  '''\n",
        "  Perform k-fold cross-validation on the given classifier.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  classifier: object\n",
        "    The classifier to be evaluated.\n",
        "\n",
        "  reviews: list\n",
        "    A list of reviews, where each review is a dictionary with keys \"cv\",\n",
        "    \"sentiment\", and \"content\". The value of \"content\" is a list of sentences\n",
        "    forming the content of a review. Each sentence is a list of tokens.\n",
        "\n",
        "  k: int, optional\n",
        "    The number of folds.\n",
        "    default: 10\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  accuracies: list\n",
        "    A list of accuracies for each fold.\n",
        "\n",
        "  average_accuracy: float\n",
        "    The average accuracy over all folds.\n",
        "  '''\n",
        "  accuracies = []\n",
        "  indices = np.arange(len(reviews))\n",
        "\n",
        "  # Perform k-fold cross-validation\n",
        "  for i in range(k):\n",
        "    classifier = classifier.__class__() # Reset classifier\n",
        "    test_indices = indices[i::k] # Round-robin selection\n",
        "    train_indices = np.delete(indices, test_indices)\n",
        "    train_reviews = [reviews[i] for i in train_indices]\n",
        "    test_reviews = [reviews[i] for i in test_indices]\n",
        "\n",
        "    classifier.fit(train_reviews)\n",
        "    results = [classifier.predict(r['content']) == r['sentiment'] for r in test_reviews]\n",
        "\n",
        "    accuracy = sum(results) / len(results)\n",
        "    accuracies.append(accuracy)\n",
        "    print(f'Fold {i+1} accuracy: {accuracy:.3f}')\n",
        "\n",
        "  average_accuracy = np.mean(accuracies)\n",
        "  print(f'Average accuracy: {average_accuracy:.3f}')\n",
        "  return accuracies, average_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qR8NOELRcgWy",
        "outputId": "7b6ab557-7ab7-4592-ff9e-84c3bb10ca0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1 accuracy: 0.775\n",
            "Fold 2 accuracy: 0.835\n",
            "Fold 3 accuracy: 0.815\n",
            "Fold 4 accuracy: 0.830\n",
            "Fold 5 accuracy: 0.775\n",
            "Fold 6 accuracy: 0.845\n",
            "Fold 7 accuracy: 0.820\n",
            "Fold 8 accuracy: 0.775\n",
            "Fold 9 accuracy: 0.820\n",
            "Fold 10 accuracy: 0.830\n",
            "Average accuracy: 0.812\n"
          ]
        }
      ],
      "source": [
        "accuracies, average_accuracy = kfold_NB(NaiveBayesClassifier(), reviews_no_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otdlsDXBNyOa"
      },
      "source": [
        "#### (Q2.6) Report the variance of the 10 accuracy scores. (0.5pt)\n",
        "\n",
        "**Please report all future results using 10-fold cross-validation now\n",
        "(unless told to use the held-out test set).** Note: you're not allowed to use a library for computing the variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoBQm1KuNzNR",
        "outputId": "38792c82-083c-425d-9fe1-1d9dd6ec7804"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variance: 0.000651\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "variance = sum((accuracies - average_accuracy)**2) / len(accuracies)\n",
        "print(f'Variance: {variance:.6f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6A2zX9_BRKm"
      },
      "source": [
        "## Features, overfitting, and the curse of dimensionality\n",
        "\n",
        "In the Bag-of-Words model, ideally we would like each distinct word in\n",
        "the text to be mapped to its own dimension in the output vector\n",
        "representation. However, real world text is messy, and we need to decide\n",
        "on what we consider to be a word. For example, is “`word`\" different\n",
        "from “`Word`\", from “`word`”, or from “`words`\"? Too strict a\n",
        "definition, and the number of features explodes, while our algorithm\n",
        "fails to learn anything generalisable. Too lax, and we risk destroying\n",
        "our learning signal. In the following section, you will learn about\n",
        "confronting the feature sparsity and the overfitting problems as they\n",
        "occur in NLP classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKK8FNt8VtcZ"
      },
      "source": [
        "### Stemming (1.5pts)\n",
        "\n",
        "To make your algorithm more robust, use stemming and hash different inflections of a word to the same feature in the BoW vector space. Please use the [Porter stemming\n",
        "    algorithm](http://www.nltk.org/howto/stem.html) from NLTK.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "NxtCul1IrBi_"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "def stem_reviews(rev, includes_pos=False):\n",
        "  '''\n",
        "  Stem the tokens in the given reviews in-place.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  rev: list\n",
        "    A list of reviews, where each review is a dictionary with keys \"cv\",\n",
        "    \"sentiment\", and \"content\". The value of \"content\" is a list of sentences\n",
        "    forming the content of a review. Each sentence is a list of features.\n",
        "\n",
        "  include_pos: bool\n",
        "    If set to true, the input includes PoS tag features.\n",
        "    default: False\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  rev: list\n",
        "    The stemmed reviews.\n",
        "  '''\n",
        "  porter_stemmer = PorterStemmer()\n",
        "  for r in rev:\n",
        "    for sentence in r['content']:\n",
        "      for i, feature in enumerate(sentence):\n",
        "        if includes_pos:\n",
        "          sentence[i] = (porter_stemmer.stem(feature[0]), feature[1])\n",
        "        else:\n",
        "          sentence[i] = porter_stemmer.stem(feature)\n",
        "  return rev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UhW-kmsQiVOl"
      },
      "outputs": [],
      "source": [
        "reviews_stemmed = deepcopy(reviews_no_pos) # Make a copy of the reviews\n",
        "reviews_stemmed = stem_reviews(reviews_stemmed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SrJ1BeLXTnk"
      },
      "source": [
        "#### (Q2.7): How does the performance of your classifier change when you use stemming on your training and test datasets? (1pt)\n",
        "Use cross-validation to evaluate the classifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYqKBOiIrInT",
        "outputId": "81091f89-164c-4efc-c189-5eb4d99fff1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1 accuracy: 0.785\n",
            "Fold 2 accuracy: 0.835\n",
            "Fold 3 accuracy: 0.800\n",
            "Fold 4 accuracy: 0.845\n",
            "Fold 5 accuracy: 0.775\n",
            "Fold 6 accuracy: 0.835\n",
            "Fold 7 accuracy: 0.810\n",
            "Fold 8 accuracy: 0.760\n",
            "Fold 9 accuracy: 0.835\n",
            "Fold 10 accuracy: 0.830\n",
            "Average accuracy: 0.811\n"
          ]
        }
      ],
      "source": [
        "# YOUR ANSWER HERE\n",
        "accuracies, average_accuracy = kfold_NB(NaiveBayesClassifier(), reviews_stemmed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkDHVq_1XUVP"
      },
      "source": [
        "#### (Q2.8) What happens to the number of features (i.e., the size of the vocabulary) when using stemming as opposed to (Q2.4)? (0.5pt)\n",
        "Give actual numbers. You can use the held-out training set to determine these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "PlbXM8cgoPYR"
      },
      "outputs": [],
      "source": [
        "def get_vocab_counts(classifier):\n",
        "  \"\"\"\n",
        "  Return the sizes of the learned vocabularies.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  classifier: NaiveBayesClassifier\n",
        "    A trained Naive Bayes classifier.\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  vocab_counts: (int, int, int)\n",
        "    sizes of full vocabulary, positive vocabulary, and negative vocabulary.\n",
        "  \"\"\"\n",
        "  vocab = Counter()\n",
        "  vocab.update(classifier.token_counter['POS'])\n",
        "  vocab.update(classifier.token_counter['NEG'])\n",
        "  return len(vocab), len(classifier.token_counter['POS']), len(classifier.token_counter['NEG'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA3vee5-rJyy",
        "outputId": "b492552f-35ec-48e2-8730-97211ddb34b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Without stemming: 45348 total features, 33105 positive features, 31042 negative features\n",
            "Without stemming: 32404 total features, 23501 positive features, 22229 negative features\n",
            "Ratio of total features without stemming to total features with stemming: 1.40\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "train_set = reviews_no_pos[:900] + reviews_no_pos[1000:1900]\n",
        "\n",
        "NB_classifier = NaiveBayesClassifier()\n",
        "NB_classifier.fit(train_set)\n",
        "counts1 = get_vocab_counts(NB_classifier)\n",
        "\n",
        "print(f'Without stemming: {counts1[0]} total features, {counts1[1]} positive features, {counts1[2]} negative features')\n",
        "\n",
        "train_set = reviews_stemmed[:900] + reviews_stemmed[1000:1900]\n",
        "\n",
        "NB_classifier = NaiveBayesClassifier()\n",
        "NB_classifier.fit(train_set)\n",
        "counts2 = get_vocab_counts(NB_classifier)\n",
        "\n",
        "print(f'Without stemming: {counts2[0]} total features, {counts2[1]} positive features, {counts2[2]} negative features')\n",
        "print(f\"Ratio of total features without stemming to total features with stemming: {counts1[0] / counts2[0]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoazfxbNV5Lq"
      },
      "source": [
        "### N-grams (1.5pts)\n",
        "\n",
        "A simple way of retaining some of the word\n",
        "order information when using bag-of-words representations is to use **n-gram** features.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHjy3I7-qWiu"
      },
      "source": [
        "#### (Q2.9) Retrain your classifier from (Q2.4) using **unigrams+bigrams** and **unigrams+bigrams+trigrams** as features. (1pt)\n",
        "Report accuracy and compare it with that of the approaches you have previously implemented. You are allowed to use NLTK to build n-grams from sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "eYuKMTOpq9jz"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "def ngram_reviews(rev, n):\n",
        "  \"\"\"\n",
        "  Creates n-grams features in-place for the reviews.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  rev: list\n",
        "    A list of reviews, where each review is a dictionary with keys \"cv\",\n",
        "    \"sentiment\", and \"content\". The value of \"content\" is a list of sentences\n",
        "    forming the content of a review. Each sentence is a list of tokens.\n",
        "\n",
        "  n: int\n",
        "    Highest number of n-grams to be included. For example, n=3 will give\n",
        "    unigrams, bigrams and trigrams as features.\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  rev: list\n",
        "    The reviews with n-grams.\n",
        "  \"\"\"\n",
        "  for r in rev:\n",
        "    for i, sentence in enumerate(r['content']):\n",
        "      m = 2\n",
        "      sentence_ngrams = sentence.copy()\n",
        "      while m <= n:\n",
        "        sentence_ngrams += list(ngrams(sentence, m))\n",
        "        m += 1\n",
        "      r['content'][i] = sentence_ngrams\n",
        "  return rev\n",
        "\n",
        "reviews_bigrams = deepcopy(reviews_stemmed)\n",
        "reviews_bigrams = ngram_reviews(reviews_bigrams, 2)\n",
        "\n",
        "reviews_trigrams = deepcopy(reviews_stemmed)\n",
        "reviews_trigrams = ngram_reviews(reviews_trigrams, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Q4LbJxcw2bl",
        "outputId": "75c1c0b8-80e5-41ed-dfba-e701a2dac98d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigram+bigram accuracies\n",
            "Fold 1 accuracy: 0.790\n",
            "Fold 2 accuracy: 0.830\n",
            "Fold 3 accuracy: 0.815\n",
            "Fold 4 accuracy: 0.880\n",
            "Fold 5 accuracy: 0.775\n",
            "Fold 6 accuracy: 0.875\n",
            "Fold 7 accuracy: 0.825\n",
            "Fold 8 accuracy: 0.825\n",
            "Fold 9 accuracy: 0.875\n",
            "Fold 10 accuracy: 0.845\n",
            "Average accuracy: 0.834\n",
            "\n",
            "Unigram+bigram+trigram accuracies\n",
            "Fold 1 accuracy: 0.805\n",
            "Fold 2 accuracy: 0.845\n",
            "Fold 3 accuracy: 0.835\n",
            "Fold 4 accuracy: 0.865\n",
            "Fold 5 accuracy: 0.755\n",
            "Fold 6 accuracy: 0.880\n",
            "Fold 7 accuracy: 0.840\n",
            "Fold 8 accuracy: 0.830\n",
            "Fold 9 accuracy: 0.855\n",
            "Fold 10 accuracy: 0.830\n",
            "Average accuracy: 0.834\n"
          ]
        }
      ],
      "source": [
        "print('Unigram+bigram accuracies')\n",
        "_ = kfold_NB(NaiveBayesClassifier(), reviews_bigrams)\n",
        "\n",
        "print('\\nUnigram+bigram+trigram accuracies')\n",
        "_ = kfold_NB(NaiveBayesClassifier(), reviews_trigrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVrGGArkrWoL"
      },
      "source": [
        "\n",
        "#### Q2.10: How many features does the BoW model have to take into account now? (0.5pt)\n",
        "How would you expect the number of features to increase theoretically (e.g., linear, square, cubed, exponential)? How does this number compare, in practice, to the number of features at (Q2.8)?\n",
        "\n",
        "Use the held-out training set once again for this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEGZ9SV8pPaa"
      },
      "source": [
        "*Answer*\n",
        "\n",
        "The number of features in a Bag-of-Words (BoW) model with n-grams would be the sum of the number of unigrams, bigrams, trigrams, etc., up to n-grams we decide to include. In theory, the number of n-grams would be:\n",
        "\n",
        "- For Unigrams: $∣V∣$ (where V is the vocabulary)\n",
        "- For Bigrams: $∣V∣\\times|V|$\n",
        "- For Trigrams: $∣V∣\\times∣V∣\\times∣V∣$\n",
        "- For n-grams: $∣V∣^n$ (where $n$ is the size of n-grams we decide to include)\n",
        "\n",
        "This would be a polynomial increase in $|V|$ but an exponential increase in $n$. For example, if we were to use unigrams, bigrams and trigrams, the number of features would in theory be $∣V∣ + ∣V∣^2 + ∣V∣^3$. In practice however, the number of features is much lower than this, because many bigrams and trigrams do not occur in the data. The reason behind this is that the number of possible bigrams and trigrams is much higher than the number of possible unigrams, and therefore the probability of a bigram or trigram occurring in the data is much lower than the probability of a unigram occurring in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z8sAJeUrdtM",
        "outputId": "828d73c2-936e-4667-de5c-c967245d2701"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigrams: 45348 total features, 33105 positive features, 31042 negative features\n",
            "Unigrams+bigrams: 407943 total features, 258927 positive features, 236327 negative features\n",
            "Unigrams+bigrams+trigrams: 1265912 total features, 748136 positive features, 672035 negative features\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "train_set = reviews_no_pos[:900] + reviews_no_pos[1000:1900]\n",
        "\n",
        "NB_classifier = NaiveBayesClassifier()\n",
        "NB_classifier.fit(train_set)\n",
        "counts1 = get_vocab_counts(NB_classifier)\n",
        "\n",
        "print(f'Unigrams: {counts1[0]} total features, {counts1[1]} positive features, {counts1[2]} negative features')\n",
        "\n",
        "train_set = reviews_bigrams[:900] + reviews_bigrams[1000:1900]\n",
        "\n",
        "NB_classifier = NaiveBayesClassifier()\n",
        "NB_classifier.fit(train_set)\n",
        "counts2 = get_vocab_counts(NB_classifier)\n",
        "\n",
        "print(f'Unigrams+bigrams: {counts2[0]} total features, {counts2[1]} positive features, {counts2[2]} negative features')\n",
        "\n",
        "train_set = reviews_trigrams[:900] + reviews_trigrams[1000:1900]\n",
        "\n",
        "NB_classifier = NaiveBayesClassifier()\n",
        "NB_classifier.fit(train_set)\n",
        "counts3 = get_vocab_counts(NB_classifier)\n",
        "\n",
        "print(f'Unigrams+bigrams+trigrams: {counts3[0]} total features, {counts3[1]} positive features, {counts3[2]} negative features')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHWKDL3YV6vh"
      },
      "source": [
        "# (3) Support Vector Machines (4pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJSYhcVaoJGt"
      },
      "source": [
        "Though simple to understand, implement, and debug, one\n",
        "major problem with the Naive Bayes classifier is that its performance\n",
        "deteriorates (becomes skewed) when it is being used with features which\n",
        "are not independent (i.e., are correlated). Another popular classifier\n",
        "that doesn’t scale as well to big data, and is not as simple to debug as\n",
        "Naive Bayes, but that doesn’t assume feature independence is the Support\n",
        "Vector Machine (SVM) classifier.\n",
        "\n",
        "You can find more details about SVMs in Chapter 7 of Bishop: Pattern Recognition and Machine Learning.\n",
        "Other sources for learning SVM:\n",
        "* http://web.mit.edu/zoya/www/SVM.pdf\n",
        "* http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf\n",
        "* https://pythonprogramming.net/support-vector-machine-intro-machine-learning-tutorial/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Use the scikit-learn implementation of\n",
        "[SVM](http://scikit-learn.org/stable/modules/svm.html) with the default parameters. (You are not expected to perform any hyperparameter tuning, but feel free to do it if you think it gives you good insights for the discussion in question 5.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LnzNtQBV8gr"
      },
      "source": [
        "#### (Q3.1): Train SVM and compare to Naive Bayes (2pts)\n",
        "\n",
        "Train an SVM classifier (sklearn.svm.LinearSVC) using the features collected for Naive Bayes. Compare the\n",
        "classification performance of the SVM classifier to that of the Naive\n",
        "Bayes classifier with smoothing.\n",
        "Use cross-validation to evaluate the performance of the classifiers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBscui8Mvoz0",
        "outputId": "56b59c2c-7d10-4dfc-f28f-afab55762972"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average accuracy on simple unigram features: 0.833\n",
            "Average accuracy on stemmed unigram features: 0.838\n",
            "Average accuracy on stemmed unigram+bigram features: 0.852\n",
            "Average accuracy on stemmed unigram+bigram+trigram features: 0.855\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "def reviews_to_Xy(review_list):\n",
        "  \"\"\"\n",
        "  Returns sparse matrix X with number of occurences per feature and sample, and\n",
        "  list y of zeros and ones representing the sentiments.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  review_list: list\n",
        "    A list of reviews, where each review is a dictionary with keys \"cv\",\n",
        "    \"sentiment\", and \"content\". The value of \"content\" is a list of sentences\n",
        "    forming the content of a review. Each sentence is a list of features.\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  X: csr_matrix\n",
        "    Sparse matrix where rows are data points, columns are features and values\n",
        "    are feature counts.\n",
        "\n",
        "  y: list\n",
        "    List of associated sentiments, 0 for negative, 1 for positive.\n",
        "  \"\"\"\n",
        "  V = dict() # Dictionary to map features to indices\n",
        "  i = 0\n",
        "  for r in review_list:\n",
        "    for sentence in r['content']:\n",
        "      for feature in sentence:\n",
        "        if feature not in V:\n",
        "          V[feature] = i # Assign index to new feature\n",
        "          i += 1 # increase index\n",
        "\n",
        "  X = np.zeros((len(review_list), len(V)), np.int16) # Sample-feature matrix\n",
        "  y = [] # Sentiments\n",
        "  for i, r in enumerate(review_list):\n",
        "    if r['sentiment'] == 'POS':\n",
        "      y.append(1)\n",
        "    else:\n",
        "      y.append(0)\n",
        "    for sentence in r['content']:\n",
        "      for feature in sentence:\n",
        "        X[i, V[feature]] += 1 # For sample i, increase feature count\n",
        "  return sp.csr_matrix(X), y # Sparse matrix otherwise my session crashes\n",
        "\n",
        "def kfold_SVM(classifier_class, X, y, k=10):\n",
        "  '''\n",
        "  Perform k-fold cross-validation on the given classifier.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  classifier: object\n",
        "    The classifier to be evaluated.\n",
        "\n",
        "  X: csr_matrix\n",
        "    (Sparse) matrix where rows are data points and columns are features.\n",
        "\n",
        "  y: list\n",
        "    List of associated outputs.\n",
        "\n",
        "  k: int, optional\n",
        "    The number of folds.\n",
        "    default: 10\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  accuracies: list\n",
        "    A list of accuracies for each fold.\n",
        "\n",
        "  average_accuracy: float\n",
        "    The average accuracy over all folds.\n",
        "  '''\n",
        "  accuracies = []\n",
        "  indices = np.arange(len(reviews))\n",
        "\n",
        "  X_folds = []\n",
        "  for i in range(k):\n",
        "    X_folds.append(X[i::k])\n",
        "\n",
        "  y_folds = []\n",
        "  for i in range(k):\n",
        "    y_folds.append(y[i::k])\n",
        "\n",
        "  # Perform k-fold cross-validation\n",
        "  for i in range(k):\n",
        "    train_X = sp.vstack(X_folds[:i] + X_folds[i+1:])\n",
        "    train_folds_y = y_folds[:i] + y_folds[i+1:] # List of folds\n",
        "    train_y = [y for fold in train_folds_y for y in fold] # Flatten list\n",
        "    test_X = X_folds[i]\n",
        "    test_y = y_folds[i]\n",
        "\n",
        "    classifier = classifier_class() # Reset classifier\n",
        "    with warnings.catch_warnings():\n",
        "      warnings.filterwarnings(\"ignore\", message=\"Liblinear failed to converge, increase the number of iterations.\")\n",
        "      classifier.fit(train_X, train_y)\n",
        "    results = classifier.predict(test_X) == test_y\n",
        "\n",
        "    accuracy = sum(results) / len(results)\n",
        "    accuracies.append(accuracy)\n",
        "    #print(f'Fold {i+1} accuracy: {accuracy:.3f}')\n",
        "\n",
        "  average_accuracy = np.mean(accuracies)\n",
        "  #print(f'Average accuracy: {average_accuracy:.3f}')\n",
        "  return accuracies, average_accuracy\n",
        "\n",
        "X, y = reviews_to_Xy(reviews_no_pos)\n",
        "_, avg = kfold_SVM(sk.svm.LinearSVC, X, y, 10)\n",
        "print(f'Average accuracy on simple unigram features: {avg:.3f}')\n",
        "\n",
        "X, y = reviews_to_Xy(reviews_stemmed)\n",
        "_, avg = kfold_SVM(sk.svm.LinearSVC, X, y, 10)\n",
        "print(f'Average accuracy on stemmed unigram features: {avg:.3f}')\n",
        "\n",
        "X, y = reviews_to_Xy(reviews_bigrams)\n",
        "_, avg = kfold_SVM(sk.svm.LinearSVC, X, y, 10)\n",
        "print(f'Average accuracy on stemmed unigram+bigram features: {avg:.3f}')\n",
        "\n",
        "X, y = reviews_to_Xy(reviews_trigrams)\n",
        "_, avg = kfold_SVM(sk.svm.LinearSVC, X, y, 10)\n",
        "print(f'Average accuracy on stemmed unigram+bigram+trigram features: {avg:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifXVWcK0V9qY"
      },
      "source": [
        "### POS disambiguation (2pts)\n",
        "\n",
        "Now add in part-of-speech features. You will find the\n",
        "movie review dataset has already been POS-tagged for you ([here](https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf) you find the tagset). Try to\n",
        "replicate the results obtained by Pang et al. (2002).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA3I82o4oWGu"
      },
      "source": [
        "#### (Q3.2) Replace your features with word+POS features, and report performance with the SVM. Use cross-validation to evaluate the classifier and compare the results with (Q3.1). Does part-of-speech information help? Explain why this may be the case. (1pt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "NOvjYe-t2Br6"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# Create dataset with lowered tokens with POS tags\n",
        "reviews_with_pos = deepcopy(reviews)\n",
        "for r in reviews_with_pos:\n",
        "  for i, sentence in enumerate(r['content']):\n",
        "    # Lower tokens, keep POS tags\n",
        "    sentence_with_pos = [(token.lower(), pos_tag) for token, pos_tag in sentence]\n",
        "    r['content'][i] = sentence_with_pos\n",
        "\n",
        "# Create dataset of reviews with stemmed tokens\n",
        "reviews_stemmed_pos = deepcopy(reviews_with_pos)\n",
        "reviews_stemmed_pos = stem_reviews(reviews_stemmed_pos, True)\n",
        "\n",
        "# Make reviews with unigram and bigram features\n",
        "reviews_bigrams_pos = deepcopy(reviews_stemmed_pos)\n",
        "reviews_bigrams_pos = ngram_reviews(reviews_bigrams_pos, 2)\n",
        "\n",
        "# Make reviews with unigram and bigram features\n",
        "reviews_trigrams_pos = deepcopy(reviews_stemmed_pos)\n",
        "reviews_trigrams_pos = ngram_reviews(reviews_trigrams_pos, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUmyDR5sGzG_",
        "outputId": "a924be8e-2e6c-4767-dbcf-f1432355b95d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average accuracy on simple unigram and POS features: 0.838\n",
            "Average accuracy on stemmed unigram and POS features: 0.833\n",
            "Average accuracy on stemmed unigram+bigram and POS features: 0.851\n",
            "Average accuracy on stemmed unigram+bigram+trigram and POS features: 0.849\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "X, y = reviews_to_Xy(reviews_with_pos)\n",
        "_, avg = kfold_SVM(sk.svm.LinearSVC, X, y, 10)\n",
        "print(f'Average accuracy on simple unigram and POS features: {avg:.3f}')\n",
        "\n",
        "X, y = reviews_to_Xy(reviews_stemmed_pos)\n",
        "_, avg = kfold_SVM(sk.svm.LinearSVC, X, y, 10)\n",
        "print(f'Average accuracy on stemmed unigram and POS features: {avg:.3f}')\n",
        "\n",
        "X, y = reviews_to_Xy(reviews_bigrams_pos)\n",
        "_, avg = kfold_SVM(sk.svm.LinearSVC, X, y, 10)\n",
        "print(f'Average accuracy on stemmed unigram+bigram and POS features: {avg:.3f}')\n",
        "\n",
        "X, y = reviews_to_Xy(reviews_trigrams_pos)\n",
        "_, avg = kfold_SVM(sk.svm.LinearSVC, X, y, 10)\n",
        "print(f'Average accuracy on stemmed unigram+bigram+trigram and POS features: {avg:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0dt_oQupUNe"
      },
      "source": [
        "*Answer*\n",
        "\n",
        "Based on our results, the POS tags don't seem to help. Accuracy has only improved a bit for the unstemmed unigram features. For the other features, accuracy has decreased slightly. This might be because it makes for sparser features. For example: if one feature without POS tag used to be somewhat common, but it has multiple different POS tags, it will be divided into multiple rarer features. These rare features are not very useful for learning and it is easy to overfit on them. This while tokens of the same form but different part-of-speech are likely to have the same associated sentiment, so it is not useful to differentiate between them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su-3w87eMW0w"
      },
      "source": [
        "#### (Q3.3) Discard all closed-class words from your data (keep only nouns, verbs, adjectives, and adverbs), and report performance. Does this help? Use cross-validation to evaluate the classifier and compare the results with (Q3.2). Are closed-class words detrimental to the classifier? Explain why this may be the case. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "CCUPlPozCYUX"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "open_classes = ['JJ', 'JJR', 'JJS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'RB',\n",
        "                'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
        "\n",
        "# Create dataset with only open-class words\n",
        "reviews_open = deepcopy(reviews)\n",
        "for r in reviews_open:\n",
        "  for i, sentence in enumerate(r['content']):\n",
        "    # Lower tokens, keep POS tags\n",
        "    sentence_open = [(token.lower(), pos_tag) for token, pos_tag in sentence\n",
        "                         if pos_tag in open_classes]\n",
        "    r['content'][i] = sentence_open\n",
        "\n",
        "# Create dataset of reviews with stemmed tokens\n",
        "reviews_stemmed_open = deepcopy(reviews_open)\n",
        "reviews_stemmed_open = stem_reviews(reviews_stemmed_open, True)\n",
        "\n",
        "# Make reviews with unigram and bigram features\n",
        "reviews_bigrams_open = deepcopy(reviews_stemmed_open)\n",
        "reviews_bigrams_open = ngram_reviews(reviews_bigrams_open, 2)\n",
        "\n",
        "# Make reviews with unigram and bigram features\n",
        "reviews_trigrams_open = deepcopy(reviews_stemmed_open)\n",
        "reviews_trigrams_open = ngram_reviews(reviews_trigrams_open, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ3JngMuJOTC",
        "outputId": "cdd78b2e-18e3-4fa8-97e1-3ccf8511bcc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average accuracies on simple open-class unigram and POS features: 0.846\n",
            "Average accuracies on stemmed open-class unigram and POS features: 0.838\n",
            "Average accuracies on stemmed open-class unigram+bigram and POS features: 0.849\n",
            "Average accuracies on stemmed open-class unigram+bigram+trigram and POS features: 0.848\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "X, y = reviews_to_Xy(reviews_open)\n",
        "_, avg = kfold_SVM(sk.svm.LinearSVC, X, y, 10)\n",
        "print(f'Average accuracies on simple open-class unigram and POS features: {avg:.3f}')\n",
        "\n",
        "X, y = reviews_to_Xy(reviews_stemmed_open)\n",
        "_, avg = kfold_SVM(sk.svm.LinearSVC, X, y, 10)\n",
        "print(f'Average accuracies on stemmed open-class unigram and POS features: {avg:.3f}')\n",
        "\n",
        "X, y = reviews_to_Xy(reviews_bigrams_open)\n",
        "_, avg = kfold_SVM(sk.svm.LinearSVC, X, y, 10)\n",
        "print(f'Average accuracies on stemmed open-class unigram+bigram and POS features: {avg:.3f}')\n",
        "\n",
        "X, y = reviews_to_Xy(reviews_trigrams_open)\n",
        "_, avg = kfold_SVM(sk.svm.LinearSVC, X, y, 10)\n",
        "print(f'Average accuracies on stemmed open-class unigram+bigram+trigram and POS features: {avg:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaxCVrs8pWSp"
      },
      "source": [
        "*Answer*\n",
        "\n",
        "Removing the closed-class words improves accuracy a bit for the unigram features, but marginally decreases accuracy for bigram and trigram features. This improvement might be because closed-class words do not convey any sentiment. This makes them useless for sentiment classification, while being a possible source of overfitting. Despite this, their influence on classification performance seems to be quite small and not necessarily detrimental, given the slightly worsened performance with bigram and trigram features. An explanation for this is that these words are extremely common in and thus equally likely for both classes, so they probably didn't play a major role in the classifier's decision anyway."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfwqOciAl2No"
      },
      "source": [
        "# (4) Discussion (max. 500 words). (5pts)\n",
        "\n",
        "> Based on your experiments, what are the effective features and techniques in sentiment analysis? What information do different features encode?\n",
        "Why is this important? What are the limitations of these features and techniques?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYuse5WLmekZ"
      },
      "source": [
        "*Write your answer here in up to 500 words (-0.25pt for >50 extra words, -0.5 points for >100 extra words, ...)*.\n",
        "\n",
        "We considered three techniques for sentiment classification. The lexicon-based approach was the simplest and also the worst performing method, with accuracies just under 70%. One of the flaws of this approach is the fact that the lexicon has been manually labelled, which can induce bias in the lexicon. As the reviews had more positive than negative words on average, we had to use a threshold based on the average difference between the number of positive and negative words. We tried an alternative threshold based on the median difference, but this did not improve performance. The lexicon also contained magnitude information regarding the sentiment of the words. Using these magnitudes slightly increased performance compared to not using them.\n",
        "\n",
        "We then moved on the Naive Bayes classifier, which basically counts how common each word is for each class and bases its decision on that. An issue arises when it stumbles upon words that were not seen in all classes during training, as this causes probability for those classes to be 0, so the words had to be skipped. This problem can be alleviated with smoothing. This smoothed version of the Naive Bayes classifier reached 81.2% accuracy with 10-fold cross-validation, greatly improving on the lexicon-based approach.\n",
        "\n",
        "We tested the Naive Bayes approach with different feature sets. We used stemmed versions of the original reviews. The idea is that this groups similar features together, making the features less sparse. However, this didn't increase accuracy on it's own. We then added bigram and trigram features. This way, the algorithm can take immediate context into account, which is crucial for determining meaning. For example: 'great' is often positive, but 'not great' or 'great disaster' are probably negative. Introducing this context information lead to a clear improvement on the previous feature sets, though the trigrams did not further improve on the bigrams. Both reached 83.4% accuracy.\n",
        "\n",
        "The final method we used was the SVM classifier, which multiplies each feature with a learned weight and adds a bias to that. We first tested it with the same features as we used for the Naive Bayes classifier. This lead to improvements in accuracy of at least 1.8 percentage points, with the trigram features reaching 85.5% accuracy. We then added part-of-speech information to the features, but this generally reduced accuracy. We thought this was because words of the same form generally point to the same sentiment, so part-of-speech information does not provide any value and only creates sparser features. Interestingly, accuracy did increase for the unstemmed unigram features, contrary to the results from Pang et al. However, it should be noted that Pang et al. used feature presence instead of feature frequency, so this comparison is problematic. We finally discarded closed-class words, as they provide little to no sentiment information. This did slightly improve performance, but only for unigram features. The highest accuracy remained the one obtained by SVM with trigram features without part-of-speech information with 85.5%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwaKwfWQhRk_"
      },
      "source": [
        "# Submission\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "aOUeaET5ijk-"
      },
      "outputs": [],
      "source": [
        "# Write your names and student numbers here:\n",
        "# Jochem Brandsema #14546620\n",
        "# Joan Velja #14950480"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A9K-H6Tii3X"
      },
      "source": [
        "**That's it!**\n",
        "\n",
        "- Check if you answered all questions fully and correctly.\n",
        "- Download your completed notebook using `File -> Download .ipynb`\n",
        "- Check if your answers are all included in the file you submit.\n",
        "- Submit your .ipynb file via *Canvas*. One submission per group."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
